{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIOj_oUioEGd"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm-polygraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AFA8PjCOpWKd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# !git clone https://github.com/IINemo/lm-polygraph.git\n",
        "!git clone https://github.com/sn0rkmaiden/lm-polygraph.git\n",
        "%cd lm-polygraph/src\n",
        "%pip install transformers rouge-score datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "# model_name = \"google/gemma-2-2b\"\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "_yBzNlyo6i5B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1"
      ],
      "metadata": {
        "id": "FcwQn_ggIpxy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# use your huggingface token HERE\n",
        "os.environ[\"HF_KEY\"] = \"\"\n",
        "login(token=os.environ.get('HF_KEY'), add_to_git_credential=False)"
      ],
      "metadata": {
        "id": "jeRcz0JW7tMk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FXJMbw5dgLVy"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, GemmaForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "d56cdd57a0d746a1b09fadc4a69ce10f",
            "169ab641b5564cd68eb09e19f09ca96c",
            "1278f8218e6e4bd09462c9d7f715680b",
            "15c886f9be8d497c838500079b3051b9",
            "95ff2cfe31744badaf9fab46a054d67b",
            "8cafb2fd334641a7ad76b3f7f0b60a8c",
            "5bb2358ae8de4d67bb32b21dc32c72c1",
            "d5480dacdb2d4f649b312eed558e43b9",
            "33138e573b6947fbb0df72c61efa5c6b",
            "73e63fe911b940ed8b84c6fc804c72d8",
            "71bb7ddf765b4c5bb253f32d620ad8be"
          ]
        },
        "id": "3bY7FWLJ6m7Z",
        "outputId": "e5aed0bd-7fed-4e53-f761-08d14e170534"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d56cdd57a0d746a1b09fadc4a69ce10f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How many fingers on a coala's foot?\"\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Who sang a song Yesterday?\"\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Кто спел песню Кукла Колдуна?\"\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Translate into Wjgnfnsdfjkn: 'I want a small cup of coffee'\"\n",
        "        }\n",
        "    ]\n",
        "]\n",
        "\n",
        "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
      ],
      "metadata": {
        "id": "tt02iyfQ6mvS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from lm_polygraph.stat_calculators.stat_calculator import StatCalculator\n",
        "from lm_polygraph.stat_calculators.embeddings import get_embeddings_from_output\n",
        "\n",
        "class OutputWrapper:\n",
        "    hidden_states = None\n",
        "    encoder_hidden_states = None\n",
        "    decoder_hidden_states = None\n",
        "\n",
        "def _gen_samples2(n_samples, model, batch, **kwargs):\n",
        "    batch_size = len(batch[\"input_ids\"])\n",
        "    logits, sequences, embeddings = (\n",
        "        [[] for _ in range(batch_size)],\n",
        "        [[] for _ in range(batch_size)],\n",
        "        [],\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for k in range(n_samples):\n",
        "            out = model.generate(**batch, **kwargs)\n",
        "            cur_logits = torch.stack(out.scores, dim=1)\n",
        "            if model.model_type == \"CausalLM\":\n",
        "                embeddings.append(\n",
        "                    {\n",
        "                        \"sample_embeddings_all_decoder\": out.hidden_states,\n",
        "                    }\n",
        "                )\n",
        "            elif model.model_type == \"Seq2SeqLM\":\n",
        "                embeddings.append(\n",
        "                    {\n",
        "                        \"sample_embeddings_all_encoder\": out.encoder_hidden_states,\n",
        "                        \"sample_embeddings_all_decoder\": out.decoder_hidden_states,\n",
        "                    }\n",
        "                )\n",
        "            for i in range(batch_size):\n",
        "                sequences[i].append(out.sequences[i])\n",
        "                logits[i].append(cur_logits[i])\n",
        "    sequences = [s for sample_seqs in sequences for s in sample_seqs]\n",
        "    return sequences, sum(logits, []), embeddings\n",
        "\n",
        "\n",
        "class SamplingGenerationCalculator2(StatCalculator):\n",
        "    \"\"\"\n",
        "    For Whitebox model (lm_polygraph.WhiteboxModel), at input texts batch calculates:\n",
        "    * sampled texts\n",
        "    * tokens of the sampled texts\n",
        "    * probabilities of the sampled tokens generation\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def meta_info() -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"\n",
        "        Returns the statistics and dependencies for the calculator.\n",
        "        \"\"\"\n",
        "\n",
        "        return [\n",
        "            \"sample_log_probs\",\n",
        "            \"sample_tokens\",\n",
        "            \"sample_texts\",\n",
        "            \"sample_log_likelihoods\",\n",
        "            \"sample_embeddings\",\n",
        "        ], []\n",
        "\n",
        "    def __init__(self, samples_n: int = 10):\n",
        "        super().__init__()\n",
        "        self.samples_n = samples_n\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        dependencies: Dict[str, np.array],\n",
        "        texts: List[str],\n",
        "        model: WhiteboxModel,\n",
        "        max_new_tokens: int = 100,\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Calculates the statistics of sampling texts.\n",
        "\n",
        "        Parameters:\n",
        "            dependencies (Dict[str, np.ndarray]): input statistics, can be empty (not used).\n",
        "            texts (List[str]): Input texts batch used for model generation.\n",
        "            model (Model): Model used for generation.\n",
        "            max_new_tokens (int): Maximum number of new tokens at model generation. Default: 100.\n",
        "        Returns:\n",
        "            Dict[str, np.ndarray]: dictionary with the following items:\n",
        "                - 'sample_texts' (List[List[str]]): `samples_n` texts for each input text in the batch,\n",
        "                - 'sample_tokens' (List[List[List[float]]]): tokenized 'sample_texts',\n",
        "                - 'sample_log_probs' (List[List[float]]): sum of the log probabilities at each token of the sampling generation.\n",
        "                - 'sample_log_likelihoods' (List[List[List[float]]]): log probabilities at each token of the sampling generation.\n",
        "                - 'sample_embeddings' (List[List[List[float]]]): embeddings from the middle layer for the last token of the sampling generation.\n",
        "        \"\"\"\n",
        "        batch: Dict[str, torch.Tensor] = model.tokenize(texts)\n",
        "        batch = {k: v.to(model.device()) for k, v in batch.items()}\n",
        "        sequences, logits, embeddings = _gen_samples2(\n",
        "            self.samples_n,\n",
        "            model,\n",
        "            batch,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            output_hidden_states=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            min_new_tokens=2,\n",
        "            do_sample=True,\n",
        "            num_beams=1,\n",
        "            num_return_sequences=1,\n",
        "            suppress_tokens=(\n",
        "                []\n",
        "                if model.generation_parameters.allow_newlines\n",
        "                else [\n",
        "                    t\n",
        "                    for t in range(len(model.tokenizer))\n",
        "                    if \"\\n\" in model.tokenizer.decode([t])\n",
        "                ]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        log_probs = [[] for _ in range(len(texts))]\n",
        "        tokens = [[] for _ in range(len(texts))]\n",
        "        texts = [[] for _ in range(len(texts))]\n",
        "        log_likelihoods = [[] for _ in range(len(texts))]\n",
        "        if model.model_type == \"Seq2SeqLM\":\n",
        "            sequences = [seq[1:] for seq in sequences]\n",
        "        for i in range(len(logits)):\n",
        "            log_prob, ll, toks = 0, [], []\n",
        "            inp_size = (\n",
        "                len(batch[\"input_ids\"][int(i / self.samples_n)])\n",
        "                if model.model_type == \"CausalLM\"\n",
        "                else 0\n",
        "            )\n",
        "            for j in range(len(sequences[i]) - inp_size):\n",
        "                cur_token = sequences[i][j + inp_size].item()\n",
        "                log_prob += logits[i][j][cur_token].item()\n",
        "                if cur_token == model.tokenizer.eos_token_id:\n",
        "                    break\n",
        "                ll.append(logits[i][j][cur_token].item())\n",
        "                toks.append(cur_token)\n",
        "\n",
        "            log_likelihoods[int(i / self.samples_n)].append(ll)\n",
        "            log_probs[int(i / self.samples_n)].append(log_prob)\n",
        "            tokens[int(i / self.samples_n)].append(toks)\n",
        "            texts[int(i / self.samples_n)].append(model.tokenizer.decode(toks))\n",
        "\n",
        "        out = OutputWrapper()\n",
        "        batch_size = len(batch[\"input_ids\"])\n",
        "        embeddings_last_token = [[] for _ in range(batch_size)]\n",
        "\n",
        "        for sample_embeddings in embeddings:\n",
        "            if model.model_type == \"CausalLM\":\n",
        "                out.hidden_states = sample_embeddings[\"sample_embeddings_all_decoder\"]\n",
        "            elif model.model_type == \"Seq2SeqLM\":\n",
        "                out.decoder_hidden_states = sample_embeddings[\n",
        "                    \"sample_embeddings_all_decoder\"\n",
        "                ]\n",
        "                out.encoder_hidden_states = sample_embeddings[\n",
        "                    \"sample_embeddings_all_encoder\"\n",
        "                ]\n",
        "            _, cur_token_embeddings = get_embeddings_from_output(\n",
        "                out,\n",
        "                batch,\n",
        "                model.model_type,\n",
        "                level=\"token\",\n",
        "                hidden_layer=int(model.model.config.num_hidden_layers // 2),\n",
        "            )\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if len(cur_token_embeddings.shape) > 2:\n",
        "                    embeddings_last_token[i].append(\n",
        "                        cur_token_embeddings[i, -1].cpu().detach().numpy()\n",
        "                    )\n",
        "                else:\n",
        "                    embeddings_last_token[i].append(\n",
        "                        cur_token_embeddings[i].cpu().detach().numpy()\n",
        "                    )\n",
        "\n",
        "        return {\n",
        "            \"sample_log_likelihoods\": log_likelihoods,\n",
        "            \"sample_log_probs\": log_probs,\n",
        "            \"sample_tokens\": tokens,\n",
        "            \"sample_texts\": texts,\n",
        "            \"sample_embeddings\": embeddings_last_token,\n",
        "        }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1zKTTP3b-R3X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lm_polygraph.stat_calculators.infer_causal_lm_calculator import InferCausalLMCalculator\n",
        "from lm_polygraph.stat_calculators.greedy_alternatives_nli import GreedyAlternativesNLICalculator\n",
        "from lm_polygraph.utils.deberta import Deberta\n",
        "from lm_polygraph.utils.model import WhiteboxModel\n",
        "from lm_polygraph.model_adapters import WhiteboxModelBasic\n",
        "from lm_polygraph.estimators import MaximumSequenceProbability, ClaimConditionedProbability, DegMat, SemanticEntropy, SAR\n",
        "from lm_polygraph.stat_calculators.cross_encoder_similarity import CrossEncoderSimilarityMatrixCalculator\n",
        "from lm_polygraph.stat_calculators.sample import SamplingGenerationCalculator\n",
        "from lm_polygraph.stat_calculators.semantic_matrix import SemanticMatrixCalculator\n",
        "from lm_polygraph.stat_calculators.semantic_classes import SemanticClassesCalculator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model_adapter = WhiteboxModelBasic(model, tokenizer, {})\n",
        "model_adapter2 = WhiteboxModel(model, tokenizer)\n",
        "\n",
        "calc_infer_llm = InferCausalLMCalculator(tokenize=False)\n",
        "nli_model = Deberta(device=device)\n",
        "nli_model.setup()\n",
        "\n",
        "calc_nli = GreedyAlternativesNLICalculator(nli_model=nli_model)\n",
        "calc_samples = SamplingGenerationCalculator2()\n",
        "calc_cross_encoder = CrossEncoderSimilarityMatrixCalculator()\n",
        "calc_semantic_matrix = SemanticMatrixCalculator(nli_model=nli_model)\n",
        "calc_semantic_classes = SemanticClassesCalculator()\n",
        "\n",
        "args_generate = {\"generation_config\" : generation_config,\n",
        "                 \"max_new_tokens\": 30}\n",
        "\n",
        "estimators = [MaximumSequenceProbability(), ClaimConditionedProbability(), SemanticEntropy()]\n",
        "# estimators = [MaximumSequenceProbability(),\n",
        "#               ClaimConditionedProbability(),\n",
        "#               DegMat(),\n",
        "#               SemanticEntropy(),\n",
        "#               SAR()]\n",
        "\n",
        "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToyzMGiw6vB5",
        "outputId": "d555f61d-c65d-4cb7-8b34-bf6161b8296b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "for batch in data_loader:\n",
        "    encoded = tokenizer(batch, padding=True, return_tensors=\"pt\")\n",
        "    deps = {\"model_inputs\": encoded}\n",
        "    deps.update(calc_infer_llm(\n",
        "        deps, texts=batch, model=model_adapter, args_generate=args_generate))\n",
        "    deps.update(calc_nli(deps, texts=batch, model=model_adapter))\n",
        "    deps.update(calc_samples(deps, texts=batch, model=model_adapter2))\n",
        "    # deps.update(calc_cross_encoder(deps, texts=batch, model=model_adapter))\n",
        "    deps.update(calc_semantic_matrix(deps, texts=batch, model=model_adapter))\n",
        "    deps.update(calc_semantic_classes(deps, texts=batch, model=model_adapter))\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(deps['greedy_tokens'])\n",
        "    ues = []\n",
        "    for estimator in estimators:\n",
        "        uncertainty_scores = estimator(deps)\n",
        "        ues.append((str(estimator), uncertainty_scores))\n",
        "\n",
        "    for i, text in enumerate(generated_texts):\n",
        "\n",
        "        print(f'Question: {messages[j][0][\"content\"]} \\nOutput: {text}')\n",
        "        for scores in ues:\n",
        "            print(f\"Uncertainty score by {scores[0]}: {scores[1][i]}\")\n",
        "        print()\n",
        "    j += 1"
      ],
      "metadata": {
        "id": "vNtWhbQ77oRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d56cdd57a0d746a1b09fadc4a69ce10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_169ab641b5564cd68eb09e19f09ca96c",
              "IPY_MODEL_1278f8218e6e4bd09462c9d7f715680b",
              "IPY_MODEL_15c886f9be8d497c838500079b3051b9"
            ],
            "layout": "IPY_MODEL_95ff2cfe31744badaf9fab46a054d67b"
          }
        },
        "169ab641b5564cd68eb09e19f09ca96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cafb2fd334641a7ad76b3f7f0b60a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_5bb2358ae8de4d67bb32b21dc32c72c1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1278f8218e6e4bd09462c9d7f715680b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5480dacdb2d4f649b312eed558e43b9",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33138e573b6947fbb0df72c61efa5c6b",
            "value": 8
          }
        },
        "15c886f9be8d497c838500079b3051b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73e63fe911b940ed8b84c6fc804c72d8",
            "placeholder": "​",
            "style": "IPY_MODEL_71bb7ddf765b4c5bb253f32d620ad8be",
            "value": " 8/8 [00:59&lt;00:00,  6.55s/it]"
          }
        },
        "95ff2cfe31744badaf9fab46a054d67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cafb2fd334641a7ad76b3f7f0b60a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb2358ae8de4d67bb32b21dc32c72c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5480dacdb2d4f649b312eed558e43b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33138e573b6947fbb0df72c61efa5c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73e63fe911b940ed8b84c6fc804c72d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71bb7ddf765b4c5bb253f32d620ad8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}